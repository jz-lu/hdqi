# Instructions for working with the data
### Jonathan Lu, on behalf of friends

## Overview
This is a dataset for a certain possible application of an algorithm which generalizes DQI
(work in progress by Stephen Jordan, Jonathan Lu, Alex Poremba, Yihui Quek, Sascha Schmidhuber).
The algorithm can, among other things, optimize commuting Hamiltonians.
(One can view DQI as optimizing a Z-type commuting Hamiltonian.)
The reduction is also to decode a classical code, which is given by the 2n x m matrix whose columns
are the symplectic representations of the terms in the Hamiltonian.
One interesting application is to, instead of generate a random k-local classical Hamiltonian,
generate a random k-local commuting Hamiltonian.
This is harder than the classical case, because there isn't a simple algorithm which immediately
generates such Hamiltonians. Instead, we generate them by picking a random k-local Pauli and
rejecting if it doesn't commute with previously chosen terms.

Once we construct the Hamiltonian, we can optimize it using our algorithm, belief propagation, and
the DQI semicircle law (which still holds in this more general case)
to see how well we did.
Separately, one thing we can do is use Gottesman's algorithm to diagonalize the Hamiltonian.
That is, we find a Clifford using a symplectic variant of Gaussian elimination which maps
every Pauli in the Hamiltonian into a X-type (my strange convention, I'm actually in the +/- basis) Pauli.
Now we can treat this Hamiltonian as a totally classical MAX-XOR-SAT problem, 
and throw it into simulated annealing (SA) to see what happens.

In classical DQI, SA obliterated DQI+BP in the analogous case, so why would we expect anything
better here? When we diagonalize, we use Gaussian elimination, and thus the classical 
version is extremely dense. The intuition is that SA gets worse and worse as the 
matrix gets denser, so we might expect that it sucks on these instances.
At the same time, there is an obvious tailored heuristic we can use to attack our approach.
Imagine the diagonalizing Clifford is `C` and the diagonalized Hamiltonian is `D` which has `m` terms.
Then every ground state of `D` is a computational basis state. Start with `0^n`.
Instead of applying local random bit flips, consider the set of Paulis `X_i, Y_i, Z_i` for all `i`. 
In the natural (un-diagonalized) basis, these are the analog of local flips.
Pass them through the Clifford `C` to get `C X_i C^\dagger`, etc., which are also Paulis, and
throw out the `X` part since they don't affect +/- states.
Use these `3n` flips as the new set of SA moves. This feels quite local and could do better.
I call SA with local bit flips `Type 1` and with Pauli flips `Type 2`.

In practice, I find that (a) our algorithm + BP + semicircle law beats both types for certain rates `m/n` 
and (b) Type 1 beats Type 2.
I don't have a deep explanation as to why either of these are true, and I certainly don't know 
if there's a much better way which again will obliterate our algorithm + BP + semicircle law.
(I am not a software engineer and the SA implementation I did is very basic.)
The goal of this dataset is to see if anyone can answer (a) and (b), and if they can beat my numerics.

## Data

The instances and BP numerics were generated by Stephen using `xortools` found [here](https://github.com/stephenjordan/xortools).
His data is found in `Stephen_raw`.
The key file is `gibbs_data.csv` where for each `n, m` and locality `k` the number of errors corrected `e` is shown.
In addition, the ratio and the actual accuracy given by the semicircle law are computed.
Also included are files of the form `cham_<n>_<m>_<k>.tsv` which are descriptions of the sampled codes.
Each row of the file is a Pauli, with the number beings the indices of the nonzero elements of its symplectic representation.
The code is taken by grabbing each row of the file, turning it into a binary vector of length `2n`, and setting the specified
indices to 1. Then horizontally gluing all these vectors into a `2n x m` matrix gives the construction.
Stephen generates 1 instance per set of parameters `n, m, k`.

For the SA numerics, see `Jonathan_SA`. 
Here, I coded up an implementation of Gottesman's diagonalization algorithm (see below for code), and then ran SA of both types. 
For sufficiently large `n`, I had to split the runs into many jobs on the computing cluster at MIT because MIT has a dumb 12 hour compute limit per job. 
This is why there are two folders `Jonathan_SA/small_cases` and `Jonathan_SA/large_cases`. The `small_cases` have data `SAStephen_TYPE<t>_m<m>n<n>k<k>_<meaningless>.npy`, which gives the SA results for a few (I think 10-20) trials for that given type, `m, n, k`. (Each trial re-samples the target vector `v` as in the DQI paper, but the Paulis/code is fixed.)
The `large_cases` are of the form `SAStephen_TYPE<t>_m<m>n<n>k<k>_<meaningless>_<trial>.npy`, where each file contains the result of
a single trial. 
There is some biasing here because some trials simply didn't finish in 12 hours because my compute sucks and/or
Python is just slow. 
The data I'm showing is a plot of what finished. 
But most runs did finish so it's probably fine.

Finally, in `Data`, I include all the raw data of the codes themselves. 
They include `DiagStephen_<m>_<n>_<k>.npy` which are the corresponding diagaonalized version of the codes into X-type Paulis, given
by `n x m` binary matrices (I cut off the bottom half of the `2n x m` matrix which are all zero).
`MovesStephen_<m>_<n>_<k>.npy` is a matrix of all the choice of moves that SA can make, which 
are consructed from passing all `3n` 1-local Paulis through the diagonalizing Clifford, and then removing the X 
part of the Pauli since only the Z part affects +/- basis states.
`CliffStephen_<m>_<n>_<k>.pkl` is a list object which is just a classical representation of the diagonalizing
Clifford circuit in the form of gates (`H, S, or CX`, `index or indices`), which you can use 
to check that the diagaonalization is correct.

## Code
My code is available on [Github](https://github.com/jz-lu/hdqi), currently private though. (Shared with Stephen and Noah.)
It's a bit messy, but there are only a few things that matter. The folder `plots` gives my data, built by `process.ipynb` (very scrappy and vibe coded).

What matters is (I) how I did the diagonalization and (II) how I did the SA.

